{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "(Download from \"https://data.mendeley.com/datasets/y7pckrw6z2/1\")\n",
    "\n",
    "Latif, Ghazanfar; Alghazo, Jaafar; Mohammad, Nazeeruddin; AlKhalaf, Roaa; AlKhalaf, Rawan (2018), “Arabic Alphabets Sign Language Dataset (ArASL)”, Mendeley Data, V1, doi: 10.17632/y7pckrw6z2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING X, Y\n",
    "from keras import models, layers\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_classes=32\n",
    "IMG_SIZE = 100\n",
    "def vectorize_data(dir_path):\n",
    "    result = []\n",
    "    labels = []\n",
    "    for label in tqdm(os.listdir(dir_path)):\n",
    "        path=os.path.join(dir_path, label)\n",
    "        for img in os.listdir(path):\n",
    "            path2 = os.path.join(path, img)\n",
    "            i = cv2.imread(path2)\n",
    "            i = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "            i = cv2.resize(cv2.imread(path2, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "            i = i.astype('float32')/255.0\n",
    "            result.append(i)\n",
    "            labels.append(label)\n",
    "    return result, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [12:38<00:00, 23.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32417,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/train')\n",
    "\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_train.shape\n",
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32417, 100, 100, 1)\n",
      "(32417,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valadation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [01:58<00:00,  3.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10797,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/val')\n",
    "\n",
    "x_val = np.array(x)\n",
    "y_val = np.array(y)\n",
    "\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_val.shape\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10797, 100, 100, 1)\n",
      "(10797,)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [03:13<00:00,  6.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10835,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/test')\n",
    "\n",
    "x_test = np.array(x)\n",
    "y_test = np.array(y)\n",
    "\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10835, 100, 100, 1)\n",
      "(10835,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir='ARDATA'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "classes_dict = {}\n",
    "classes_inv_dict = []\n",
    "for i, class_label in enumerate(os.listdir(train_dir)):\n",
    "    classes_dict[class_label] = i\n",
    "    classes_inv_dict.append(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ain', 'al', 'aleff', 'bb', 'dal', 'dha', 'dhad', 'fa', 'gaaf', 'ghain', 'ha', 'haa', 'jeem', 'kaaf', 'khaa', 'la', 'laam', 'meem', 'nun', 'ra', 'saad', 'seen', 'sheen', 'ta', 'taa', 'thaa', 'thal', 'toot', 'waw', 'ya', 'yaa', 'zay']\n"
     ]
    }
   ],
   "source": [
    "print(classes_inv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes_inv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "num_classes=len(classes_dict)\n",
    "keys, inv = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "# TRAINING\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_train_new = vals[inv]\n",
    "y_train_new_cat = to_categorical(y_train_new, num_classes)\n",
    "\n",
    "\n",
    "#VALADATION\n",
    "keys, inv = np.unique(y_val, return_inverse=True)\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_val_new = vals[inv]\n",
    "y_val_new_cat = to_categorical(y_val_new, num_classes)\n",
    "\n",
    "\n",
    "# SHUFFLE\n",
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "x_new,y_new = unison_shuffled_copies(x_train,y_train_new_cat)\n",
    "x_newval,y_newval = unison_shuffled_copies(x_val,y_val_new_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CREATION\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "IMG_SIZE = 100\n",
    "\n",
    "num_classes = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (2,2), input_shape=(IMG_SIZE, IMG_SIZE, 1), activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 99, 99, 16)        80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               819328    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 846,672\n",
      "Trainable params: 846,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'saved-best-model/the_best-model', monitor='val_loss', mode='min', \n",
    "    save_weights_only=False , save_best_only=True, verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16209/16209 [==============================] - ETA: 0s - loss: 1.4236 - accuracy: 0.5842\n",
      "Epoch 00001: val_loss improved from inf to 0.34150, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 218s 13ms/step - loss: 1.4236 - accuracy: 0.5842 - val_loss: 0.3415 - val_accuracy: 0.9053\n",
      "Epoch 2/10\n",
      "16208/16209 [============================>.] - ETA: 0s - loss: 0.3648 - accuracy: 0.8937\n",
      "Epoch 00002: val_loss improved from 0.34150 to 0.26239, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 214s 13ms/step - loss: 0.3648 - accuracy: 0.8937 - val_loss: 0.2624 - val_accuracy: 0.9277\n",
      "Epoch 3/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9323\n",
      "Epoch 00003: val_loss improved from 0.26239 to 0.20830, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 215s 13ms/step - loss: 0.2317 - accuracy: 0.9323 - val_loss: 0.2083 - val_accuracy: 0.9472\n",
      "Epoch 4/10\n",
      "16207/16209 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9479\n",
      "Epoch 00004: val_loss improved from 0.20830 to 0.19727, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 213s 13ms/step - loss: 0.1745 - accuracy: 0.9479 - val_loss: 0.1973 - val_accuracy: 0.9498\n",
      "Epoch 5/10\n",
      "16207/16209 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9580 ETA: 0s - loss: 0.1399 - accuracy - ETA: 0s - loss: 0.1398 - accuracy\n",
      "Epoch 00005: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 217s 13ms/step - loss: 0.1396 - accuracy: 0.9580 - val_loss: 0.1982 - val_accuracy: 0.9544\n",
      "Epoch 6/10\n",
      "16207/16209 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9625\n",
      "Epoch 00006: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 217s 13ms/step - loss: 0.1220 - accuracy: 0.9625 - val_loss: 0.1985 - val_accuracy: 0.9558\n",
      "Epoch 7/10\n",
      "16205/16209 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9671\n",
      "Epoch 00007: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 223s 14ms/step - loss: 0.1091 - accuracy: 0.9671 - val_loss: 0.7786 - val_accuracy: 0.8664\n",
      "Epoch 8/10\n",
      "16207/16209 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9707\n",
      "Epoch 00008: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 227s 14ms/step - loss: 0.0998 - accuracy: 0.9707 - val_loss: 0.2698 - val_accuracy: 0.9554\n",
      "Epoch 9/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9736\n",
      "Epoch 00009: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 220s 14ms/step - loss: 0.0887 - accuracy: 0.9736 - val_loss: 0.2375 - val_accuracy: 0.9553\n",
      "Epoch 10/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9747\n",
      "Epoch 00010: val_loss did not improve from 0.19727\n",
      "16209/16209 [==============================] - 216s 13ms/step - loss: 0.0867 - accuracy: 0.9747 - val_loss: 0.2981 - val_accuracy: 0.9580\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_new, y_new, \n",
    "          epochs = 10, \n",
    "          validation_data = (x_newval, y_newval), \n",
    "          shuffle = True, \n",
    "          batch_size = 2,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014/1014 [==============================] - 23s 23ms/step - loss: 0.0208 - accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0208049975335598, 0.994200587272644]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 8s 23ms/step - loss: 0.2981 - accuracy: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.298123300075531, 0.9579513072967529]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_newval,y_newval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, inv = np.unique(y_test, return_inverse=True)\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_test_new = vals[inv]\n",
    "y_test_new_cat = to_categorical(y_test_new,num_classes=len(classes_dict))\n",
    "x_testnew,y_testnew = unison_shuffled_copies(x_test,y_test_new_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 8s 23ms/step - loss: 0.3609 - accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36090943217277527, 0.9544993042945862]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_testnew, y_testnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"aramodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone\n",
    "from cvzone.SelfiSegmentationModule import SelfiSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img):\n",
    "    if not cv2.imwrite('img.jpg', img):\n",
    "        raise IOException('Failed to save image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from keras.models import load_model\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "from translate import Translator\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "\n",
    "model = load_model(\"aramodel.h5\")\n",
    "IMG_SIZE = 100\n",
    "top, right, bottom, left = 10, 350, 225, 590\n",
    "SAVE_PATH = \"C:/Users/Andalus/Final\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound \n",
    "def voice(vo):\n",
    "    language = 'en'\n",
    "    myobj = gTTS(text=string, lang=language, slow=False)\n",
    "    myobj.save(\"hi.mp3\")\n",
    "    os.system(\"mpg321 hi.mp3\")\n",
    "    playsound(\"hi.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = 1\n",
    "arabic_labels = {\"ain\":'ع',\n",
    "\"al\":'ال',           \n",
    "\"aleff\":'أ',\n",
    "\"bb\":'ب',\n",
    "\"dal\":'د',\n",
    "\"dha\":'ط',\n",
    "\"dhad\":\"ض\",\n",
    "\"fa\":\"ف\",\n",
    "\"gaaf\":'ق',\n",
    "\"ghain\":'غ',\n",
    "\"ha\":'ه',\n",
    "\"haa\":'ح',\n",
    "\"jeem\":'ج',\n",
    "\"kaaf\":'ك',\n",
    "\"khaa\":'خ',\n",
    "\"la\":'لا',        \n",
    "\"laam\":'ل',\n",
    "\"meem\":'م',\n",
    "\"nun\":\"ن\",\n",
    "\"ra\":'ر',\n",
    "\"saad\":'ص',\n",
    "\"seen\":'س',\n",
    "\"sheen\":\"ش\",\n",
    "\"ta\":'ت',\n",
    "\"taa\":'ط',\n",
    "\"thaa\":\"ث\",\n",
    "\"thal\":\"ذ\",\n",
    "\"toot\":'ة',\n",
    "\"waw\":'و',\n",
    "\"ya\":\"ى\",\n",
    "\"yaa\":'ئ',\n",
    "\"zay\":'ز'}\n",
    "\n",
    "\n",
    "num_class = 32\n",
    "out = \"\"\n",
    "text = \"\"\n",
    "inv_dictonary = dict(map(reversed, arabic_labels.items()))\n",
    "classes_inv_dict = ['ain', 'al', 'aleff', 'bb', 'dal', 'dha', 'dhad', 'fa', 'gaaf', 'ghain', 'ha', 'haa', 'jeem', \n",
    "                    'kaaf', 'khaa', 'la', 'laam', 'meem', 'nun', 'ra', 'saad', 'seen', 'sheen', 'ta', 'taa', 'thaa',\n",
    "                    'thal', 'toot', 'waw', 'ya', 'yaa', 'zay']\n",
    "string = \"\"\n",
    "\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "success, frame = camera.read()\n",
    "segmentor=SelfiSegmentation()\n",
    "#time_start = time.time()\n",
    "\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(static_image_mode=False,\n",
    "                      max_num_hands=2,\n",
    "                      min_detection_confidence=0.5,\n",
    "                      min_tracking_confidence=0.5)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "pTime = 0\n",
    "cTime = 0\n",
    "\n",
    "\n",
    "while success:\n",
    "    \n",
    "    '''\n",
    "    time_new = time.time()\n",
    "    time_diff = time_new - time_start\n",
    "    if time_diff > 3:\n",
    "        ....\n",
    "        time_start = time_new\n",
    "    '''\n",
    "    \n",
    "    gray_frame = cv2.flip(frame, 1)\n",
    "#     cv2.rectangle(gray_frame, (left, top), (right, bottom), (0,255,0), 2)\n",
    "#     roi = gray_frame[top:bottom, right:left]\n",
    "    roi = gray_frame\n",
    "#     roi=segmentor.removeBG(roi,(0,0,0))\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"Segmentation\", roi)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 39:\n",
    "        break\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # PREDICTION\n",
    "    roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    roi = np.expand_dims(roi, axis=-1)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    p = model.predict(roi)\n",
    "    p2 = np.argmax(p)\n",
    "    out = classes_inv_dict[p2]\n",
    "    out = arabic_labels[out]\n",
    "    \n",
    "    #PREDICTION ENDS\n",
    "    \n",
    "    #reshape\n",
    "    reshaped_text = arabic_reshaper.reshape(out)\n",
    "    bidi_text = get_display(reshaped_text) \n",
    "    reshaped_string = arabic_reshaper.reshape(string)\n",
    "    bidi_string = get_display(reshaped_string)\n",
    "    fontpath = \"arial.ttf\" \n",
    "    font = ImageFont.truetype(fontpath, 32)\n",
    "    img_pil = Image.fromarray(gray_frame)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text((22, 34),bidi_text, font = font)\n",
    "    draw.text((22, 64),bidi_string, font = font)\n",
    "    img = np.array(img_pil)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #landmark\n",
    "    results = hands.process(gray_frame)\n",
    "    #print(results.multi_hand_landmarks)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            for id, lm in enumerate(handLms.landmark):\n",
    "                #print(id,lm)\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x *w), int(lm.y*h)\n",
    "                #if id ==0:\n",
    "                cv2.circle(img, (cx,cy), 3, (255,0,255), cv2.FILLED)\n",
    "\n",
    "            mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)\n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    cv2.imshow(\"Sign Language Translator\", img)\n",
    "    #endlandmak\n",
    "    \n",
    "    \n",
    "    \n",
    "    if key == ord('a'):\n",
    "        string =string+out\n",
    "   \n",
    "    if key == ord('v'):\n",
    "        voice(string)\n",
    "         \n",
    "    if key ==ord('b'):                            # Backspace\n",
    "        string= string[:-1]\n",
    "        \n",
    "    if key == ord('s'):                           #space\n",
    "        string= string +\" \"\n",
    "       \n",
    "    if key == ord('r'):                            #clear\n",
    "#         roi = frame[top:bottom, right:left]\n",
    "        roi = frame\n",
    "        string = \"\"\n",
    "\n",
    "    if key == ord('c'):                             #screen\n",
    "        img_name = \"data{}.png\".format(img_count)\n",
    "        img_count+=1\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, img_name), gray_frame)\n",
    "        \n",
    "\n",
    "    success, frame = camera.read()\n",
    "      \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
