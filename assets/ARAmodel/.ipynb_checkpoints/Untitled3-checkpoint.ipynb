{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "(Download from \"https://data.mendeley.com/datasets/y7pckrw6z2/1\")\n",
    "\n",
    "Latif, Ghazanfar; Alghazo, Jaafar; Mohammad, Nazeeruddin; AlKhalaf, Roaa; AlKhalaf, Rawan (2018), “Arabic Alphabets Sign Language Dataset (ArASL)”, Mendeley Data, V1, doi: 10.17632/y7pckrw6z2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING X, Y\n",
    "from keras import models, layers\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "num_classes=32\n",
    "IMG_SIZE = 100\n",
    "def vectorize_data(dir_path):\n",
    "    result = []\n",
    "    labels = []\n",
    "    for label in tqdm(os.listdir(dir_path)):\n",
    "        path=os.path.join(dir_path, label)\n",
    "        for img in os.listdir(path):\n",
    "            path2 = os.path.join(path, img)\n",
    "            i = cv2.imread(path2)\n",
    "            i = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "            i = cv2.resize(cv2.imread(path2, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "            i = i.astype('float32')/255.0\n",
    "            result.append(i)\n",
    "            labels.append(label)\n",
    "    return result, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [08:00<00:00, 15.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32417,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/train')\n",
    "\n",
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_train.shape\n",
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32417, 100, 100, 1)\n",
      "(32417,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valadation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:55<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10797,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/val')\n",
    "\n",
    "x_val = np.array(x)\n",
    "y_val = np.array(y)\n",
    "\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_val.shape\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10797, 100, 100, 1)\n",
      "(10797,)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [01:28<00:00,  2.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10835,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = vectorize_data('ARDATA/test')\n",
    "\n",
    "x_test = np.array(x)\n",
    "y_test = np.array(y)\n",
    "\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10835, 100, 100, 1)\n",
      "(10835,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir='ARDATA'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "classes_dict = {}\n",
    "classes_inv_dict = []\n",
    "for i, class_label in enumerate(os.listdir(train_dir)):\n",
    "    classes_dict[class_label] = i\n",
    "    classes_inv_dict.append(class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "num_classes=len(classes_dict)\n",
    "keys, inv = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "# TRAINING\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_train_new = vals[inv]\n",
    "y_train_new_cat = to_categorical(y_train_new, num_classes)\n",
    "\n",
    "\n",
    "#VALADATION\n",
    "keys, inv = np.unique(y_val, return_inverse=True)\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_val_new = vals[inv]\n",
    "y_val_new_cat = to_categorical(y_val_new, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# SHUFFLE\n",
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "x_new,y_new = unison_shuffled_copies(x_train,y_train_new_cat)\n",
    "x_newval,y_newval = unison_shuffled_copies(x_val,y_val_new_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CREATION\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "IMG_SIZE = 100\n",
    "\n",
    "num_classes = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (2,2), input_shape=(IMG_SIZE, IMG_SIZE, 1), activation='relu'))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2) ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 99, 99, 16)        80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               819328    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 846,672\n",
      "Trainable params: 846,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'saved-best-model/the_best-model', monitor='val_loss', mode='min', \n",
    "    save_weights_only=False , save_best_only=True, verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16209/16209 [==============================] - ETA: 0s - loss: 1.1173 - accuracy: 0.6699\n",
      "Epoch 00001: val_loss improved from inf to 0.31848, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 249s 15ms/step - loss: 1.1173 - accuracy: 0.6699 - val_loss: 0.3185 - val_accuracy: 0.9103\n",
      "Epoch 2/10\n",
      "16205/16209 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.9099\n",
      "Epoch 00002: val_loss improved from 0.31848 to 0.21306, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 202s 12ms/step - loss: 0.3074 - accuracy: 0.9099 - val_loss: 0.2131 - val_accuracy: 0.9469\n",
      "Epoch 3/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.1994 - accuracy: 0.9417\n",
      "Epoch 00003: val_loss improved from 0.21306 to 0.19929, saving model to saved-best-model\\the_best-model\n",
      "INFO:tensorflow:Assets written to: saved-best-model\\the_best-model\\assets\n",
      "16209/16209 [==============================] - 206s 13ms/step - loss: 0.1993 - accuracy: 0.9418 - val_loss: 0.1993 - val_accuracy: 0.9527\n",
      "Epoch 4/10\n",
      "16207/16209 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9543\n",
      "Epoch 00004: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 201s 12ms/step - loss: 0.1531 - accuracy: 0.9543 - val_loss: 0.2040 - val_accuracy: 0.9502\n",
      "Epoch 5/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9615\n",
      "Epoch 00005: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 201s 12ms/step - loss: 0.1259 - accuracy: 0.9615 - val_loss: 0.2057 - val_accuracy: 0.9525\n",
      "Epoch 6/10\n",
      "16205/16209 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9684\n",
      "Epoch 00006: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 201s 12ms/step - loss: 0.1044 - accuracy: 0.9684 - val_loss: 0.2227 - val_accuracy: 0.9606\n",
      "Epoch 7/10\n",
      "16209/16209 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9731\n",
      "Epoch 00007: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 202s 12ms/step - loss: 0.0897 - accuracy: 0.9731 - val_loss: 0.2253 - val_accuracy: 0.9561\n",
      "Epoch 8/10\n",
      "16208/16209 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9759\n",
      "Epoch 00008: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 200s 12ms/step - loss: 0.0806 - accuracy: 0.9759 - val_loss: 0.2412 - val_accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9769\n",
      "Epoch 00009: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 202s 12ms/step - loss: 0.0757 - accuracy: 0.9768 - val_loss: 0.2614 - val_accuracy: 0.9590\n",
      "Epoch 10/10\n",
      "16206/16209 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9791\n",
      "Epoch 00010: val_loss did not improve from 0.19929\n",
      "16209/16209 [==============================] - 203s 13ms/step - loss: 0.0739 - accuracy: 0.9791 - val_loss: 0.3322 - val_accuracy: 0.9562\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_new, y_new, \n",
    "          epochs = 10, \n",
    "          validation_data = (x_newval, y_newval), \n",
    "          shuffle = True, \n",
    "          batch_size = 2,\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014/1014 [==============================] - 23s 22ms/step - loss: 0.0200 - accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.020041488111019135, 0.994077205657959]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 8s 22ms/step - loss: 0.3322 - accuracy: 0.9562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3321878910064697, 0.9561915397644043]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_newval,y_newval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, inv = np.unique(y_test, return_inverse=True)\n",
    "vals = np.array([classes_dict[key] for key in keys])\n",
    "y_test_new = vals[inv]\n",
    "y_test_new_cat = to_categorical(y_test_new,num_classes=len(classes_dict))\n",
    "x_testnew,y_testnew = unison_shuffled_copies(x_test,y_test_new_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 8s 22ms/step - loss: 0.3961 - accuracy: 0.9515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3961392641067505, 0.9514536261558533]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_testnew, y_testnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Train.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Train.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone\n",
    "from cvzone.SelfiSegmentationModule import SelfiSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img):\n",
    "    if not cv2.imwrite('img.jpg', img):\n",
    "        raise IOException('Failed to save image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from keras.models import load_model\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "from translate import Translator\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "\n",
    "model = load_model(\"Train.model\")\n",
    "IMG_SIZE = 100\n",
    "top, right, bottom, left = 10, 350, 225, 590\n",
    "SAVE_PATH = \"C:/Users/Andalus/Final\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound \n",
    "def voice(vo):\n",
    "    language = 'en'\n",
    "    myobj = gTTS(text=string, lang=language, slow=False)\n",
    "    myobj.save(\"hi.mp3\")\n",
    "    os.system(\"mpg321 hi.mp3\")\n",
    "    playsound(\"hi.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_count = 1\n",
    "arabic_labels = {\"ain\":'ع',\n",
    "\"al\":'ال',           \n",
    "\"aleff\":'أ',\n",
    "\"bb\":'ب',\n",
    "\"dal\":'د',\n",
    "\"dha\":'ط',\n",
    "\"dhad\":\"ض\",\n",
    "\"fa\":\"ف\",\n",
    "\"gaaf\":'ق',\n",
    "\"ghain\":'غ',\n",
    "\"ha\":'ه',\n",
    "\"haa\":'ح',\n",
    "\"jeem\":'ج',\n",
    "\"kaaf\":'ك',\n",
    "\"khaa\":'خ',\n",
    "\"la\":'لا',        \n",
    "\"laam\":'ل',\n",
    "\"meem\":'م',\n",
    "\"nun\":\"ن\",\n",
    "\"ra\":'ر',\n",
    "\"saad\":'ص',\n",
    "\"seen\":'س',\n",
    "\"sheen\":\"ش\",\n",
    "\"ta\":'ت',\n",
    "\"taa\":'ط',\n",
    "\"thaa\":\"ث\",\n",
    "\"thal\":\"ذ\",\n",
    "\"toot\":'ة',\n",
    "\"waw\":'و',\n",
    "\"ya\":\"ى\",\n",
    "\"yaa\":'ئ',\n",
    "\"zay\":'ز'}\n",
    "\n",
    "\n",
    "num_class = 32\n",
    "out = \"\"\n",
    "text = \"\"\n",
    "inv_dictonary = dict(map(reversed, arabic_labels.items()))\n",
    "string = \"\"\n",
    "\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "success, frame = camera.read()\n",
    "segmentor=SelfiSegmentation()\n",
    "#time_start = time.time()\n",
    "color_mapping = {\n",
    "    \"handrecogation/Hand.xml\": (255, 0, 0),\n",
    "    \"handrecogation/Hand_0_Gesture.xml\": (0, 255, 0)\n",
    "}\n",
    "classifiers = {}\n",
    "\n",
    "for classifier in color_mapping.keys():\n",
    "    classifiers[classifier] = cv2.CascadeClassifier(classifier)\n",
    "while success:\n",
    "    \n",
    "    '''\n",
    "    time_new = time.time()\n",
    "    time_diff = time_new - time_start\n",
    "    if time_diff > 3:\n",
    "        ....\n",
    "        time_start = time_new\n",
    "    '''\n",
    "    \n",
    "    gray_frame = cv2.flip(frame, 1)\n",
    "#     cv2.rectangle(gray_frame, (left, top), (right, bottom), (0,255,0), 2)\n",
    "#    roi = gray_frame[top:bottom, right:left]\n",
    "    roi = gray_frame\n",
    "    roi=segmentor.removeBG(roi,(0,0,0))\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"Segmentation\", roi)\n",
    "    \n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 39:\n",
    "        break\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PREDICTION\n",
    "    roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    roi = np.expand_dims(roi, axis=-1)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    p = model.predict(roi)\n",
    "    p2 = np.argmax(p)\n",
    "    out = classes_inv_dict[p2]\n",
    "    out = arabic_labels[out]\n",
    "\n",
    "    for classifier in color_mapping.keys():\n",
    "        clf = classifiers[classifier]\n",
    "        r_color = color_mapping[classifier]\n",
    "        d_rects = clf.detectMultiScale(gray_frame, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in d_rects:\n",
    "            cv2.rectangle(gray_frame, (x, y), (x+w, y+h), r_color)\n",
    "            \n",
    "    #PREDICTION ENDS\n",
    "    reshaped_text = arabic_reshaper.reshape(out)\n",
    "    bidi_text = get_display(reshaped_text) \n",
    "    reshaped_string = arabic_reshaper.reshape(string)\n",
    "    bidi_string = get_display(reshaped_string)\n",
    "    fontpath = \"arial.ttf\" \n",
    "    font = ImageFont.truetype(fontpath, 32)\n",
    "    img_pil = Image.fromarray(gray_frame)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text((22, 34),bidi_text, font = font)\n",
    "    draw.text((22, 64),bidi_string, font = font)\n",
    "    img = np.array(img_pil)\n",
    "    \n",
    "#     font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#     cv2.putText(gray_frame,out, (22,34), font, 1, (200,255,255), 2, cv2.LINE_AA)\n",
    "#     cv2.putText(gray_frame,string, (22,64), font, 1, (200,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Sign Language Translator\", img)\n",
    "    \n",
    "    \n",
    "    if key == ord('a'):\n",
    "        string =bidi_string+bidi_text\n",
    "   \n",
    "    if key == ord('v'):\n",
    "        voice(string)\n",
    "         \n",
    "    if key ==ord('b'):                            # Backspace\n",
    "        string= string[:-1]\n",
    "        \n",
    "    if key == ord('s'):                           #space\n",
    "        string= string +\" \"\n",
    "       \n",
    "    if key == ord('r'):                            #clear\n",
    "        roi = frame[top:bottom, right:left]\n",
    "        string = \"\"\n",
    "\n",
    "    if key == ord('c'):                             #screen\n",
    "        img_name = \"data{}.png\".format(img_count)\n",
    "        img_count+=1\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, img_name), gray_frame)\n",
    "        \n",
    "\n",
    "    success, frame = camera.read()\n",
    "      \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
